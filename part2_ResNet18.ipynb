{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "663f1198-1397-4df5-981a-c9c0449a51bd",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "- https://keras.io/guides/transfer_learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139aac1c-7cba-472a-b3a5-b9f759790a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 15:20:01.768520: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768486801.827229  301731 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768486801.845106  301731 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768486801.978814  301731 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768486801.978841  301731 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768486801.978844  301731 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768486801.978847  301731 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-15 15:20:01.993503: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/enrico/ComputerVision/.venv/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import keras\n",
    "import pandas as pd\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "seed = 42 # for testability purposes\n",
    "keras.utils.set_random_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3d64885-d5bb-4fd9-9034-6b9ddf074bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_to_change(img, size):\n",
    "    if img.shape[0] <= img.shape[1]:\n",
    "        return (256, int(np.floor(256 / img.shape[0] * img.shape[1])))\n",
    "    else:\n",
    "        return (int(np.floor(256 / img.shape[1] * img.shape[0])), 256)\n",
    "\n",
    "def get_random_crop(img, cropX=224, cropY=224):\n",
    "    max_x = img.shape[0] - cropX\n",
    "    max_y = img.shape[1] - cropY\n",
    "\n",
    "    x = np.random.randint(0, max_x)\n",
    "    y = np.random.randint(0, max_y)\n",
    "\n",
    "    crop = img[x: x + cropX, y: y + cropY]\n",
    "    \n",
    "    return crop\n",
    "\n",
    "def load_data(url_directory, url_file, size=256, crop_dim=224, random_crop=False, central_crop=False, return_original=False):\n",
    "    X = []\n",
    "    y = []\n",
    "    X_original = []\n",
    "    \n",
    "    with open(url_directory + url_file) as f:\n",
    "        for line in f:\n",
    "            line_split = line.split(\",\")\n",
    "            img = cv2.imread(url_directory + line_split[0])\n",
    "            img_resized = cv2.resize(img, size_to_change(img, size))\n",
    "            lbl = int(line_split[2].strip())\n",
    "\n",
    "            if random_crop:\n",
    "                crop = get_random_crop(img_resized, crop_dim, crop_dim) # is it fine even for val and test?\n",
    "            elif central_crop: # center crop\n",
    "                img_center = (img_resized.shape[0] // 2, img_resized.shape[1] // 2)\n",
    "                crop_dim_half = crop_dim // 2\n",
    "                crop = img_resized[\n",
    "                    img_center[0] - crop_dim_half : img_center[0] - crop_dim_half + crop_dim,\n",
    "                    img_center[1] - crop_dim_half : img_center[1] - crop_dim_half + crop_dim\n",
    "                    ]\n",
    "            else:\n",
    "                crop = img_resized[:crop_dim, :crop_dim]\n",
    "\n",
    "            X.append(crop)\n",
    "            y.append(lbl)\n",
    "\n",
    "            if return_original:\n",
    "                img_resized_original = img_resized[:size, :size]\n",
    "                X_original.append(img_resized_original)\n",
    "\n",
    "    X = np.stack(X, axis=0)\n",
    "    y = np.array(y)\n",
    "\n",
    "    if return_original:\n",
    "        return X, y, np.stack(X_original, axis=0)\n",
    "    else:\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ea2e96-29c4-4262-b2e2-444bf5a716ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_jittering(img, brightness_range=(-60, 60), saturation_range=(-60, 60)):\n",
    "    rng = np.random.default_rng()\n",
    "    img_hsv = cv2.cvtColor(img.copy(), cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    brightness = rng.integers(brightness_range[0], brightness_range[1])\n",
    "    saturation = rng.integers(saturation_range[0], saturation_range[1])\n",
    "\n",
    "    img_hsv[:, :, 1] = np.clip(img_hsv[:, :, 1] + saturation, 0, 255)\n",
    "    img_hsv[:, :, 2] = np.clip(img_hsv[:, :, 2] + brightness, 0, 255)\n",
    "\n",
    "    img_hsv = img_hsv.astype(np.uint8)\n",
    "\n",
    "    return cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "def contrast_stretching(img, linear_contrast=True, gamma_correction=False, gamma_correction_parameter=1.0):\n",
    "    img_copy = img.copy()\n",
    "    \n",
    "    if linear_contrast:\n",
    "        p_min = np.percentile(img_copy, 5)\n",
    "        p_max = np.percentile(img_copy, 95)\n",
    "\n",
    "        img_copy = np.clip((255/(p_max - p_min)) * (img_copy - p_min), 0, 255).astype(np.uint8)\n",
    "\n",
    "    if gamma_correction:\n",
    "        img_copy = np.clip(255 * np.power(img_copy/255, gamma_correction_parameter), 0, 255).astype(np.uint8)\n",
    "\n",
    "    return img_copy\n",
    "\n",
    "def cutout(img, size=64, color=50):\n",
    "    rng = np.random.default_rng()\n",
    "    img_copy = img.copy()\n",
    "\n",
    "    x = rng.integers(0, img.shape[0] - size)\n",
    "    y = rng.integers(0, img.shape[1] - size)\n",
    "\n",
    "    img_copy[x : x + size, y : y + size] = color\n",
    "    return img_copy\n",
    "\n",
    "def zoom(img, zoom_center=None, zoom_pixel=50):\n",
    "    img_copy = img.copy()\n",
    "    zoom_tot = img.shape[0] // 2 - zoom_pixel\n",
    "    \n",
    "    if zoom_center == None:\n",
    "        cx = img_copy.shape[0] // 2\n",
    "        cy = img_copy.shape[1] // 2\n",
    "    else:\n",
    "        cx = zoom_center[0]\n",
    "        cy = zoom_center[y]\n",
    "    \n",
    "    pts1 = np.float32([(cx - zoom_tot, cy - zoom_tot), (cx + zoom_tot, cy - zoom_tot), (cx + zoom_tot, cy + zoom_tot), (cx - zoom_tot, cy + zoom_tot)])\n",
    "    pts2 = np.float32([(0, 0), (img_copy.shape[0], 0), (img_copy.shape[0], img_copy.shape[1]), (0, img_copy.shape[1])])\n",
    "    M = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "    dst = cv2.warpPerspective(img_copy, M, (img.shape[0], img.shape[1]))\n",
    "    \n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a654168-9e9d-4756-9d6d-f7aac854a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_directory = \"../GroceryStoreDataset/dataset/\"\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_train, y_train, X_train_original = load_data(url_directory, \"train.txt\", random_crop=True, return_original=True)\n",
    "X_val, y_val = load_data(url_directory, \"val.txt\", central_crop=True)\n",
    "X_test, y_test = load_data(url_directory, \"test.txt\", central_crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1b4be5d-5a39-4f3b-a534-70838662a30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classes = pd.read_csv(url_directory + \"classes.csv\")\n",
    "df_coarse = df_classes.loc[:, ['Coarse Class Name (str)', 'Coarse Class ID (int)']].drop_duplicates().values\n",
    "labels_coarse = {i: lbl for lbl, i in df_coarse}\n",
    "n_classes_coarse = len(labels_coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8b8db60-280e-491c-bf31-bd17fac9a74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13200, 224, 224, 3) (13200,)\n"
     ]
    }
   ],
   "source": [
    "X_train_augmented = []\n",
    "y_train_augmented = []\n",
    "n_crop = 1\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "for idx, img in enumerate(X_train_original):\n",
    "    lbl = y_train[idx]\n",
    "\n",
    "    img_base = get_random_crop(img)\n",
    "    X_train_augmented.append(img_base)\n",
    "    y_train_augmented.append(lbl)\n",
    "\n",
    "    img_base_zoom = zoom(img_base)\n",
    "    X_train_augmented.append(img_base_zoom)\n",
    "    y_train_augmented.append(lbl)\n",
    "\n",
    "    if rng.integers(2) == 0: \n",
    "        img_base_flip = cv2.flip(img_base, 1)\n",
    "        X_train_augmented.append(img_base_flip)\n",
    "        y_train_augmented.append(lbl)\n",
    "    else:\n",
    "        img_base_flip = cv2.flip(img_base, 0)\n",
    "        X_train_augmented.append(img_base_flip)\n",
    "        y_train_augmented.append(lbl)\n",
    "\n",
    "    img_base_cutout = cutout(img_base_flip)\n",
    "    X_train_augmented.append(img_base_cutout)\n",
    "    y_train_augmented.append(lbl)\n",
    "\n",
    "    for j in range(0, n_crop):\n",
    "        img_new = get_random_crop(img)\n",
    "        img_new = contrast_stretching(img_new)\n",
    "        img_new = color_jittering(img_new)\n",
    "        X_train_augmented.append(img_new)\n",
    "        y_train_augmented.append(lbl)\n",
    "\n",
    "        #if rng.integers(2) == 0:\n",
    "        #    img_new_flip = cv2.flip(img_new, 1) \n",
    "        #    X_train_augmented.append(img_new_flip)\n",
    "        #    y_train_augmented.append(lbl)\n",
    "        #else:\n",
    "        #    img_new_flip = cv2.flip(img_new, 0)\n",
    "        #    X_train_augmented.append(img_new_flip)\n",
    "        #    y_train_augmented.append(lbl)\n",
    "\n",
    "        #img_new_cutout = cutout(img_new_flip)\n",
    "        #X_train_augmented.append(img_new_cutout)\n",
    "        #y_train_augmented.append(lbl)\n",
    "\n",
    "\n",
    "X_train_augmented = np.stack(X_train_augmented, axis=0)\n",
    "y_train_augmented = np.array(y_train_augmented)\n",
    "print(X_train_augmented.shape, y_train_augmented.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3a48dda-62f0-407f-aea8-9c355cce23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#train_ds = tf.data.Dataset.from_tensor_slices((X_train_augmented, y_train_augmented))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356c5a47-8015-449f-acdb-fb3acfe74990",
   "metadata": {},
   "source": [
    "#### MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fd2f7a6-23f7-47c0-bbed-66590d9d31d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 15:20:18.804730: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2026-01-15 15:20:18.804782: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=\"-1\"\n",
      "2026-01-15 15:20:18.804789: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA\n",
      "2026-01-15 15:20:18.804794: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n",
      "2026-01-15 15:20:18.804800: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: acer\n",
      "2026-01-15 15:20:18.804803: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: acer\n",
      "2026-01-15 15:20:18.805014: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 580.95.5\n",
      "2026-01-15 15:20:18.805048: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 580.95.5\n",
      "2026-01-15 15:20:18.805052: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 580.95.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ res_net_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">11,186,112</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResNetBackbone</span>)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">22,059</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ res_net_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │    \u001b[38;5;34m11,186,112\u001b[0m │\n",
       "│ (\u001b[38;5;33mResNetBackbone\u001b[0m)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m)             │        \u001b[38;5;34m22,059\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,208,171</span> (42.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,208,171\u001b[0m (42.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">22,059</span> (86.17 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m22,059\u001b[0m (86.17 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,186,112</span> (42.67 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m11,186,112\u001b[0m (42.67 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras_hub\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "# First, instantiate a base model with pre-trained weights\n",
    "base_model = keras_hub.models.ResNetBackbone.from_preset(\"resnet_18_imagenet\")\n",
    "\n",
    "# Then, freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create a new model on top\n",
    "inputs = keras.Input(shape=X_train_augmented.shape[1:])\n",
    "scale_layer = keras.layers.Rescaling(scale=1./255)\n",
    "x = scale_layer(inputs)\n",
    "\n",
    "x = base_model(x, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = keras.layers.Dense(units=n_classes_coarse, activation='softmax')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c4b82e-2848-4139-9dbc-efdb2d7c6af1",
   "metadata": {},
   "source": [
    "#### MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f175f47-adca-48fd-bba0-a61af5481a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 15:20:22.123309: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1986969600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129/413\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:56\u001b[0m 621ms/step - accuracy: 0.1821 - loss: 3.1451"
     ]
    }
   ],
   "source": [
    "batch_size = 32 #128\n",
    "epochs = 15\n",
    "\n",
    "# Train the model on new data\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train_augmented,\n",
    "    y_train_augmented,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd69dab-7def-4217-8ffa-877eedc8c209",
   "metadata": {},
   "source": [
    "#### FINE TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc2eeb-0a05-4b5f-acef-25d0a7a7b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4fee00-5aba-440b-988d-68a5173d6cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_fine_tuning = 7\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train_augmented,\n",
    "    y_train_augmented,\n",
    "    epochs=epochs_fine_tuning,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb4329-7014-4ec2-8384-9f149d34500d",
   "metadata": {},
   "source": [
    "#### MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7637be-47ca-48c1-8f35-5a5e674b76a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
